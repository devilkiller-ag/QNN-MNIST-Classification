{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NiiJ5kC_BsEe",
        "outputId": "af685ee3-162f-4dfe-8e31-7dfb9f42149c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting classiq\n",
            "  Downloading classiq-0.35.0-py3-none-any.whl (358 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m358.7/358.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
            "Collecting ConfigArgParse<2.0.0,>=1.5.3 (from classiq)\n",
            "  Downloading ConfigArgParse-1.7-py3-none-any.whl (25 kB)\n",
            "Collecting Pyomo<6.6,>=6.5 (from classiq)\n",
            "  Downloading Pyomo-6.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx<1,>=0.23.0 (from classiq)\n",
            "  Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: keyring<24.0.0,>=23.5.0 in /usr/lib/python3/dist-packages (from classiq) (23.5.0)\n",
            "Requirement already satisfied: matplotlib<4.0.0,>=3.4.3 in /usr/local/lib/python3.10/dist-packages (from classiq) (3.7.1)\n",
            "Collecting networkx<3.0.0,>=2.5.1 (from classiq)\n",
            "  Downloading networkx-2.8.8-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numexpr<3.0.0,>=2.7.3 in /usr/local/lib/python3.10/dist-packages (from classiq) (2.8.8)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.20.1 in /usr/local/lib/python3.10/dist-packages (from classiq) (1.23.5)\n",
            "Collecting packaging<22.0,>=21.3 (from classiq)\n",
            "  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas<2.0.0,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from classiq) (1.5.3)\n",
            "Requirement already satisfied: plotly<6.0.0,>=5.7.0 in /usr/local/lib/python3.10/dist-packages (from classiq) (5.15.0)\n",
            "Requirement already satisfied: pydantic<2.0.0,>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from classiq) (1.10.13)\n",
            "Collecting sympy<1.11.0,>=1.9.0 (from classiq)\n",
            "  Downloading sympy-1.10.1-py3-none-any.whl (6.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tabulate<1,>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from classiq) (0.9.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->classiq) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->classiq) (2023.11.17)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->classiq)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->classiq) (3.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->classiq) (1.3.0)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->classiq)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.4.3->classiq) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.4.3->classiq) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.4.3->classiq) (4.47.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.4.3->classiq) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.4.3->classiq) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.4.3->classiq) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.4.3->classiq) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.0.0,>=1.4.0->classiq) (2023.3.post1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly<6.0.0,>=5.7.0->classiq) (8.2.3)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.0.0,>=1.9.1->classiq) (4.5.0)\n",
            "Collecting ply (from Pyomo<6.6,>=6.5->classiq)\n",
            "  Downloading ply-3.11-py2.py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy<1.11.0,>=1.9.0->classiq) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib<4.0.0,>=3.4.3->classiq) (1.16.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->classiq) (1.2.0)\n",
            "Installing collected packages: ply, torchinfo, sympy, Pyomo, packaging, networkx, h11, ConfigArgParse, httpcore, httpx, classiq\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.12\n",
            "    Uninstalling sympy-1.12:\n",
            "      Successfully uninstalled sympy-1.12\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 23.2\n",
            "    Uninstalling packaging-23.2:\n",
            "      Successfully uninstalled packaging-23.2\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.2.1\n",
            "    Uninstalling networkx-3.2.1:\n",
            "      Successfully uninstalled networkx-3.2.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed ConfigArgParse-1.7 Pyomo-6.5.0 classiq-0.35.0 h11-0.14.0 httpcore-1.0.2 httpx-0.26.0 networkx-2.8.8 packaging-21.3 ply-3.11 sympy-1.10.1 torchinfo-1.8.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "packaging"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install classiq torchinfo tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGaHOTWrVYKb",
        "outputId": "531840e9-cf78-4862-9b70-8efed9aa8944"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/classiq/_internals/authentication/token_manager.py:82: UserWarning: Device is already registered.\n",
            "Generating a new refresh token should only be done if the current refresh token is compromised.\n",
            "To do so, set the overwrite parameter to true\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import classiq\n",
        "\n",
        "classiq.authenticate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "VNxmFaE1B9fw"
      },
      "outputs": [],
      "source": [
        "\"\"\"_summary_\n",
        "Linear Entanglement Quantum Model for MNIST Data Classification with three linear entanglement layers of RXX, RYY, and RZZ.\n",
        "\"\"\"\n",
        "\n",
        "from classiq import create_model, QFunc, QArray, QBit, Output, allocate, RX, RY, RZ, RZZ, RXX, RYY, CZ\n",
        "\n",
        "@QFunc\n",
        "def encoding(q: QArray[QBit]) -> None:\n",
        "    \"\"\"\n",
        "    This function encodes the input data into the qubits. This input data is a 4x4 image pixel values\n",
        "    converted into angle for rotation gates (RX, RY, RZ, RX) in form of a 16x1 vector.\n",
        "    We encode 4 pixels per qubit.\n",
        "\n",
        "    Args:\n",
        "        q (QArray[QBit]): Array of four Qubits to encode the input data into.\n",
        "    \"\"\"\n",
        "    RX(theta=\"input_0\", target=q[0]) # Pixel 0 on Qubit 0\n",
        "    RY(theta=\"input_1\", target=q[0]) # Pixel 1 on Qubit 0\n",
        "    RZ(theta=\"input_2\", target=q[0]) # Pixel 2 on Qubit 0\n",
        "    RX(theta=\"input_3\", target=q[0]) # Pixel 3 on Qubit 0\n",
        "\n",
        "    RX(theta=\"input_4\", target=q[1]) # Pixel 4 on Qubit 1\n",
        "    RY(theta=\"input_5\", target=q[1]) # Pixel 5 on Qubit 1\n",
        "    RZ(theta=\"input_6\", target=q[1]) # Pixel 6 on Qubit 1\n",
        "    RX(theta=\"input_7\", target=q[1]) # Pixel 7 on Qubit 1\n",
        "\n",
        "    RX(theta=\"input_8\", target=q[2]) # Pixel 8 on Qubit 2\n",
        "    RY(theta=\"input_9\", target=q[2]) # Pixel 9 on Qubit 2\n",
        "    RZ(theta=\"input_10\", target=q[2]) # Pixel 10 on Qubit 2\n",
        "    RX(theta=\"input_11\", target=q[2]) # Pixel 11 on Qubit 2\n",
        "\n",
        "    RX(theta=\"input_12\", target=q[3]) # Pixel 12 on Qubit 3\n",
        "    RY(theta=\"input_13\", target=q[3]) # Pixel 13 on Qubit 3\n",
        "    RZ(theta=\"input_14\", target=q[3]) # Pixel 14 on Qubit 3\n",
        "    RX(theta=\"input_15\", target=q[3]) # Pixel 15 on Qubit 3\n",
        "\n",
        "@QFunc\n",
        "def mixing(q: QArray[QBit]) -> None:\n",
        "    \"\"\"\n",
        "    This function performs the mixing operation on the qubits.\n",
        "    This is done by applying a series of RZZ, RXX, RYY gates to form a\n",
        "    ring connection.\n",
        "\n",
        "    Args:\n",
        "        q (QArray[QBit]): Array of four Qubits to apply the mixing operation on.\n",
        "    \"\"\"\n",
        "    RZZ(theta=\"weight_0\", target=q[0:2])\n",
        "    RZZ(theta=\"weight_1\", target=q[1:3])\n",
        "    RZZ(theta=\"weight_2\", target=q[2:4])\n",
        "\n",
        "    RXX(theta=\"weight_4\", target=q[0:2])\n",
        "    RXX(theta=\"weight_5\", target=q[1:3])\n",
        "    RXX(theta=\"weight_6\", target=q[2:4])\n",
        "\n",
        "    RYY(theta=\"weight_8\", target=q[0:2])\n",
        "    RYY(theta=\"weight_9\", target=q[1:3])\n",
        "    RYY(theta=\"weight_10\", target=q[2:4])\n",
        "\n",
        "@QFunc\n",
        "def cz_block(q: QArray[QBit]) -> None:\n",
        "    \"\"\"\n",
        "    This function applies CZ gates between each qubit.\n",
        "\n",
        "    Args:\n",
        "        q (QArray[QBit]): Array of four Qubits to apply the entanglement operation on.\n",
        "    \"\"\"\n",
        "    CZ(control=q[0], target=q[1])\n",
        "    CZ(control=q[1], target=q[2])\n",
        "    CZ(control=q[2], target=q[3])\n",
        "\n",
        "@QFunc\n",
        "def main(res: Output[QArray[QBit]]) -> None:\n",
        "    \"\"\"\n",
        "    This is the main function from which model will be created.\n",
        "    It calls the other functions to perform the encoding, mixing and entanglement.\n",
        "\n",
        "    Args:\n",
        "        res (Output[QArray[QBit]]): Output QArray of QBits from which the model will be created.\n",
        "    \"\"\"\n",
        "    allocate(4, res)\n",
        "    encoding(q=res)\n",
        "    mixing(q=res)\n",
        "    cz_block(q=res)\n",
        "\n",
        "def linear_entanglement_r3_quantum_model():\n",
        "    model = create_model(main)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "iWw1V0mqCSYe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "from classiq.execution import execute_qnn\n",
        "from classiq.synthesis import SerializedQuantumProgram\n",
        "from classiq.applications.qnn import QLayer\n",
        "from classiq.applications.qnn.types import (\n",
        "    MultipleArguments,\n",
        "    SavedResult,\n",
        "    ResultsCollection,\n",
        ")\n",
        "\n",
        "def execute_fn(quantum_program: SerializedQuantumProgram, arguments: MultipleArguments) -> ResultsCollection:\n",
        "    return execute_qnn(quantum_program, arguments)\n",
        "\n",
        "def post_process_fn(result: SavedResult) -> torch.Tensor:\n",
        "    counts: dict = result.value.counts\n",
        "\n",
        "    # Calculate logits from counts\n",
        "    logits: float = torch.zeros(16)\n",
        "    for key, value in counts.items():\n",
        "        logits[int(key, 2)] = value\n",
        "\n",
        "    # Trim the logits from length 16 to length 10 since we have only 10 labels\n",
        "    trimmed_logits = logits[:2]\n",
        "\n",
        "    # Calculate prediction probabilities from logits by normalizing it\n",
        "    pred_probs = torch.nn.functional.normalize(trimmed_logits, dim=0)\n",
        "\n",
        "    # Convert the prediction probabilities into prediction labels\n",
        "    pred_labels = torch.argmax(pred_probs)\n",
        "\n",
        "    ### WRITE COUNTS, OUTPUT LOGITS, PRED PROBS, PRED LABELS to a file\n",
        "    # output_file = open(\"post_process_output.txt\", \"a\")\n",
        "    # print(\"----------------------------------------------------------------------------------------------------------------------------------------------\", file=output_file)\n",
        "    # print(f\"COUNTS:: \\n {counts} \\n\", file=output_file)\n",
        "    # print(f\"LOGITS:: \\n {logits} \\n\", file=output_file)\n",
        "    # print(f\"TRIMMED LOGITS:: \\n {trimmed_logits} \\n\", file=output_file)\n",
        "    # print(f\"PREDICTION PROBABILITIES:: \\n {pred_probs} \\n\", file=output_file)\n",
        "    # print(f\"PREDICTION LABELS:: \\n {pred_labels} \\n\", file=output_file)\n",
        "    # output_file.close()\n",
        "\n",
        "    return pred_probs.clone().detach()\n",
        "\n",
        "class QNN(torch.nn.Module):\n",
        "    def __init__(self, quantum_program, execute, post_process, *args, **kwargs) -> None:\n",
        "        super().__init__()\n",
        "        self.qlayer = QLayer(\n",
        "            quantum_program,\n",
        "            execute=execute,\n",
        "            post_process=post_process,\n",
        "            *args,\n",
        "            **kwargs\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.qlayer(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7CYd_InsCVTF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from typing import Callable,Optional\n",
        "\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "\n",
        "NUM_WORKERS = os.cpu_count()\n",
        "\n",
        "def create_dataloaders_from_folders(\n",
        "    train_dir: str,\n",
        "    test_dir: str,\n",
        "    batch_size: int,\n",
        "    transform: Optional[Callable] = None,\n",
        "    target_transform: Optional[Callable] = None,\n",
        "    num_workers: int = NUM_WORKERS\n",
        "):\n",
        "    \"\"\"Creates training and testing DataLoaders.\n",
        "\n",
        "    Takes in a training directory and testing directory path and turns\n",
        "    them into PyTorch Datasets and then into PyTorch DataLoaders.\n",
        "\n",
        "    Args:\n",
        "        train_dir: Path to training directory.\n",
        "        test_dir: Path to testing directory.\n",
        "        transform: function having torchvision transforms to perform on training and testing data.\n",
        "        target_transform: function having torchvision transforms to perform on training and testing data labels.\n",
        "        batch_size: Number of samples per batch in each of the DataLoaders.\n",
        "        num_workers: An integer for number of workers per DataLoader.\n",
        "\n",
        "    Returns:\n",
        "        A tuple of (train_dataloader, test_dataloader, class_names).\n",
        "        Where class_names is a list of the target classes.\n",
        "        Example usage:\n",
        "        train_dataloader, test_dataloader, class_names = \\\n",
        "            = create_dataloaders_from_folders(train_dir=path/to/train_dir,\n",
        "                                            test_dir=path/to/test_dir,\n",
        "                                            transform=some_data_transform_function,\n",
        "                                            target_transform=some_label_transform_function,\n",
        "                                            batch_size=32,\n",
        "                                            num_workers=4)\n",
        "    \"\"\"\n",
        "    # Use ImageFolder to create dataset(s)\n",
        "    train_data = datasets.ImageFolder(train_dir, transform=transform, target_transform=target_transform)\n",
        "    test_data = datasets.ImageFolder(test_dir, transform=transform, target_transform=target_transform)\n",
        "\n",
        "    # Get class names\n",
        "    class_names = train_data.classes\n",
        "\n",
        "    # Turn images into data loaders\n",
        "    train_dataloader = DataLoader(\n",
        "        train_data,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "    test_dataloader = DataLoader(\n",
        "        test_data,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "\n",
        "    return train_dataloader, test_dataloader, class_names\n",
        "\n",
        "def create_mnist_dataloaders(\n",
        "    batch_size: int,\n",
        "    root: str = \"data\",\n",
        "    transform: Optional[Callable] = None,\n",
        "    target_transform: Optional[Callable] = None,\n",
        "    num_workers: int = NUM_WORKERS,\n",
        "    create_subset: bool = False,\n",
        "    subset_size:int = 64\n",
        "):\n",
        "    \"\"\"Creates training and testing DataLoaders.\n",
        "\n",
        "    Creates PyTorch Dataloaders from PyTorch MNIST Dataset.\n",
        "\n",
        "    Args:\n",
        "        root: folder name in which data will be downloaded.\n",
        "        transform: torchvision transforms to perform on training and testing data.\n",
        "        target_transform: function having torchvision transforms to perform on training and testing data labels.\n",
        "        batch_size: Number of samples per batch in each of the DataLoaders.\n",
        "        num_workers: An integer for number of workers per DataLoader.\n",
        "        create_subset: If True, it create a dataloaders from small subset of data.\n",
        "        subset_size: Size of the subset of data. Defaults to 64.\n",
        "\n",
        "    Returns:\n",
        "        A tuple of (train_dataloader, test_dataloader, class_names).\n",
        "        Where class_names is a list of the target classes.\n",
        "        Example usage:\n",
        "        train_dataloader, test_dataloader, class_names = \\\n",
        "            = create_mnist_dataloaders(root=\"data\"\n",
        "                                    transform=some_data_transform_function,\n",
        "                                    target_transform=some_label_transform_function,\n",
        "                                    batch_size=32,\n",
        "                                    num_workers=4)\n",
        "    \"\"\"\n",
        "    # Setup training data\n",
        "    train_data = datasets.MNIST(\n",
        "        root=root,\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=transform,\n",
        "        target_transform=target_transform\n",
        "    )\n",
        "\n",
        "    # Setup testing data\n",
        "    test_data = datasets.MNIST(\n",
        "        root=root,\n",
        "        train=False,\n",
        "        download=True,\n",
        "        transform=transform,\n",
        "        target_transform=target_transform\n",
        "    )\n",
        "\n",
        "    # Get class names\n",
        "    class_names = train_data.classes\n",
        "\n",
        "    # Create subsets of the datasets\n",
        "    if create_subset:\n",
        "        train_data = Subset(train_data, range(subset_size))\n",
        "        test_data = Subset(test_data, range(subset_size))\n",
        "\n",
        "    # Turn datasets into iterables (batches)\n",
        "    train_dataloader = DataLoader(train_data,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "    )\n",
        "    test_dataloader = DataLoader(test_data,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "    )\n",
        "\n",
        "    return train_dataloader, test_dataloader, class_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "9n7og5-OCbbq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "def input_transform(image):\n",
        "    \"\"\"\n",
        "    The input MNIST images are all 28 × 28 px. This function will firstly center-crop\n",
        "    them to 24 × 24 and then down-sample them to 4 × 4 for MNIST. Then we convert\n",
        "    the image pixels into angles for passing them into Rotation gates later for encoding.\n",
        "    \"\"\"\n",
        "    image = transforms.Grayscale(num_output_channels=1)(image)\n",
        "    image = transforms.ToTensor()(image)\n",
        "    image = transforms.CenterCrop(24)(image)\n",
        "    image = transforms.Resize(size = (4,4), antialias=True)(image)\n",
        "    image = image.squeeze()\n",
        "    image_pixels = torch.flatten(image)\n",
        "    angles = torch.sqrt(image_pixels / 256)\n",
        "\n",
        "    return angles\n",
        "\n",
        "def target_transform(label):\n",
        "    label_tensor = torch.LongTensor([label])\n",
        "    one_hot_label = torch.nn.functional.one_hot(label_tensor, 10)\n",
        "    return one_hot_label.squeeze()\n",
        "\n",
        "def target_transform_bin(label):\n",
        "    label_tensor = torch.LongTensor([label])\n",
        "    one_hot_label = torch.nn.functional.one_hot(label_tensor, 2)\n",
        "    return one_hot_label.squeeze()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0s4pgrfwCe9Z"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from datetime import datetime\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def create_writer(experiment_name: str,\n",
        "                  model_name: str,\n",
        "                  extra: str=None) -> torch.utils.tensorboard.writer.SummaryWriter():\n",
        "    \"\"\"Creates a torch.utils.tensorboard.writer.SummaryWriter() instance saving to a specific log_dir.\n",
        "\n",
        "    log_dir is a combination of runs/timestamp/experiment_name/model_name/extra.\n",
        "\n",
        "    Where timestamp is the current date in YYYY-MM-DD format.\n",
        "\n",
        "    Args:\n",
        "        experiment_name (str): Name of experiment.\n",
        "        model_name (str): Name of model.\n",
        "        extra (str, optional): Anything extra to add to the directory. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        torch.utils.tensorboard.writer.SummaryWriter(): Instance of a writer saving to log_dir.\n",
        "\n",
        "    Example usage:\n",
        "        # Create a writer saving to \"runs/2022-06-04/data_10_percent/leqm3/5_epochs/\"\n",
        "        writer = create_writer(experiment_name=\"data_10_percent\",\n",
        "                                model_name=\"leqm3\",\n",
        "                                extra=\"5_epochs\")\n",
        "        # The above is the same as:\n",
        "        writer = SummaryWriter(log_dir=\"runs/2022-06-04/data_10_percent/leqm3/5_epochs/\")\n",
        "    \"\"\"\n",
        "\n",
        "    # Get timestamp of current date (all experiments on certain day live in same folder)\n",
        "    timestamp = datetime.now().strftime(\"%Y-%m-%d\") # returns current date in YYYY-MM-DD format\n",
        "\n",
        "    if extra:\n",
        "        # Create log directory path\n",
        "        log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name, extra)\n",
        "    else:\n",
        "        log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name)\n",
        "\n",
        "    print(f\"[INFO] Created SummaryWriter, saving to: {log_dir}...\")\n",
        "    return SummaryWriter(log_dir=log_dir)\n",
        "\n",
        "def write_train_results(\n",
        "    experiment_name,\n",
        "    model_name,\n",
        "    epochs,\n",
        "    results,\n",
        "):\n",
        "    output_dir = \"outputs/train_results/\"\n",
        "\n",
        "    # Check if the directory exists, and create it if not\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    output_file_name = f\"{experiment_name}_{model_name}_epochs_{epochs}.csv\"\n",
        "    file_path = f\"outputs/train_results/{output_file_name}\"\n",
        "\n",
        "    data = {\n",
        "        'Epoch': list(range(1, len(results['train_loss']) + 1)),\n",
        "        'Train Loss': results['train_loss']\n",
        "    }\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    if os.path.exists(file_path):\n",
        "        # If it exists, append data\n",
        "        df.to_csv(file_path, mode='a', index=False, header=False)\n",
        "    else:\n",
        "        df.to_csv(file_path, mode='w', index=False, header=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jt4zUdmdCi7D"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Contains various utility functions for PyTorch model training and saving.\n",
        "\"\"\"\n",
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "def save_model(model: torch.nn.Module, target_dir: str, model_name: str):\n",
        "    \"\"\"Saves a PyTorch model to a target directory.\n",
        "\n",
        "    Args:\n",
        "        model: A target PyTorch model to save.\n",
        "        target_dir: A directory for saving the model to.\n",
        "        model_name: A filename for the saved model. Should include\n",
        "            either \".pth\" or \".pt\" as the file extension.\n",
        "\n",
        "    Example usage:\n",
        "        save_model(model=model_0,\n",
        "                    target_dir=\"models\",\n",
        "                    model_name=\"05_going_modular_tingvgg_model.pth\")\n",
        "    \"\"\"\n",
        "    # Create target directory\n",
        "    target_dir_path = Path(target_dir)\n",
        "    target_dir_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Create model save path\n",
        "    assert model_name.endswith(\".pth\") or model_name.endswith(\n",
        "        \".pt\"\n",
        "    ), \"model_name should end with '.pt' or '.pth'\"\n",
        "    model_save_path = target_dir_path / model_name\n",
        "\n",
        "    # Save the model state_dict()\n",
        "    print(f\"[INFO] Saving model to: {model_save_path}\")\n",
        "    torch.save(obj=model.state_dict(), f=model_save_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "fFxxC8NwCmtI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def test(\n",
        "    model: nn.Module,\n",
        "    data_loader: DataLoader,\n",
        "    atol=0,\n",
        "    device: str = 'cpu',\n",
        ") -> float:\n",
        "    num_correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Put the model in eval mode\n",
        "    model.eval()\n",
        "\n",
        "    # Turn on inference mode context manager\n",
        "    with torch.inference_mode():\n",
        "        for data, labels in data_loader:\n",
        "            # Send data to GPU\n",
        "            data, labels = data.to(device), labels.to(device)\n",
        "\n",
        "            # 1. Forward pass: Let the model predict\n",
        "            predictions = model(data)\n",
        "\n",
        "            # Get a tensor of booleans, indicating if each label is close to the real label\n",
        "            is_prediction_correct = torch.isclose(predictions.argmax(dim=1), labels.argmax(dim=1), atol=atol)\n",
        "\n",
        "            ### WRITE OUTPUT TO A FILE\n",
        "            # output_file = open(\"test_loop_output.txt\", \"a\")\n",
        "            # print(\"----------------------------------------------------------------------------------------------------------------------------------------------\", file=output_file)\n",
        "            # print(f\"LABELS:: \\n {labels} \\n\", file=output_file)\n",
        "            # print(f\"PREDICTIONS:: \\n {predictions} \\n\", file=output_file)\n",
        "            # print(f\"IS PREDICTIONS CORRECT:: \\n {is_prediction_correct} \\n\", file=output_file)\n",
        "            # output_file.close()\n",
        "\n",
        "            # Count the amount of `True` predictions\n",
        "            num_correct += is_prediction_correct.sum().item()\n",
        "\n",
        "            # Count the total evaluations\n",
        "            #   the first dimension of `labels` is `batch_size`\n",
        "            total += labels.size(0)\n",
        "\n",
        "    # Calculate the accuracy\n",
        "    accuracy = float(num_correct) / float(total)\n",
        "    print(f\"Test Accuracy of the model: {accuracy * 100:.2f}%\")\n",
        "    return accuracy * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "xmPIVNlRCq6t"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "from typing import Dict, List\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "def train(\n",
        "    model: nn.Module,\n",
        "    data_loader: DataLoader,\n",
        "    loss_fn: nn.modules.loss._Loss,\n",
        "    optimizer: optim.Optimizer,\n",
        "    writer: torch.utils.tensorboard.writer.SummaryWriter,\n",
        "    epochs: int = 20,\n",
        "    device: str = 'cpu',\n",
        ") -> Dict[str, List]:\n",
        "    model.to(device)\n",
        "\n",
        "    # Setup train loss value\n",
        "    train_loss = 0\n",
        "\n",
        "    # Create empty results dictionary\n",
        "    results = {\n",
        "        \"train_loss\": [],\n",
        "    }\n",
        "\n",
        "    # Loop through training steps for a number of epochs\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        print(f\"Epoch: {epoch}\\n----------\")\n",
        "        for batch, (data, label) in enumerate(data_loader):\n",
        "            # Send data to device (GPU or CPU)\n",
        "            data, label = data.to(device), label.to(device)\n",
        "\n",
        "            # 1. Forward pass\n",
        "            output = model(data).to(device)\n",
        "\n",
        "            # 2. Calculate loss\n",
        "            loss = loss_fn(output, label)\n",
        "            train_loss += loss\n",
        "\n",
        "            # 3. Optimizer zero grad\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # 4. Loss backward\n",
        "            loss.backward()\n",
        "\n",
        "            # 5. Optimizer step\n",
        "            optimizer.step()\n",
        "\n",
        "        # Calculate loss per epoch and print out what's happening\n",
        "        train_loss /= len(data_loader)\n",
        "\n",
        "        # Print out what's happening\n",
        "        print(\n",
        "            f\"Epoch: {epoch+1} | \"\n",
        "            f\"Train loss: {train_loss:.5f}\"\n",
        "        )\n",
        "\n",
        "        # Update results dictionary\n",
        "        results[\"train_loss\"].append(train_loss.detach().item())\n",
        "\n",
        "        ### Experiment Tracking ###\n",
        "        # See if there's a writer, if so, log to it\n",
        "        if writer:\n",
        "            # Add loss results to SummaryWriter\n",
        "            writer.add_scalars(\n",
        "                main_tag=\"Loss\",\n",
        "                tag_scalar_dict={\"train_loss\": train_loss,},\n",
        "                global_step=epoch\n",
        "            )\n",
        "\n",
        "            # Close the writer\n",
        "            writer.close()\n",
        "\n",
        "    # Return the filled results at the end of the epochs\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ynh_fWlyBhs7"
      },
      "source": [
        "### Imports and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "zk41ETM5Bhs9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import classiq\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchinfo import summary\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"../..\") # Add the parent directory to the sys.path list\n",
        "\n",
        "# from models.leqm3 import linear_entanglement_r3_quantum_model\n",
        "# from models.qnn import execute_fn, post_process_fn, QNN\n",
        "\n",
        "# from scripts.helper import create_writer, write_train_results\n",
        "# from scripts.data_setup import create_mnist_dataloaders\n",
        "# from scripts.data_transforms import input_transform, target_transform\n",
        "# from scripts.train import train\n",
        "# from scripts.test import test\n",
        "# from scripts.save_model import save_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ZBufxv1gBhs-"
      },
      "outputs": [],
      "source": [
        "## Authenticate Classiq\n",
        "# classiq.authenticate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "zRj81-sXBhs-",
        "outputId": "b768c6b5-5753-4b9d-d688-5a54ef4df205"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "## For setting up device agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device = 'cpu'\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "AeZrbf2ZBhs_"
      },
      "outputs": [],
      "source": [
        "# ## Clear Output Files\n",
        "# post_process_output_file = open(\"post_process_output.txt\", \"w\")\n",
        "# print(\"-----------------------------------------------------------------------------------------------------------------\", file=post_process_output_file)\n",
        "# print(\"--------------------------------------------POST PROCESS OUTPUT--------------------------------------------------\", file=post_process_output_file)\n",
        "# print(\"-----------------------------------------------------------------------------------------------------------------\", file=post_process_output_file)\n",
        "# post_process_output_file.close()\n",
        "\n",
        "# test_loop_output_file = open(\"test_loop_output.txt\", \"w\")\n",
        "# print(\"-----------------------------------------------------------------------------------------------------------------\", file=test_loop_output_file)\n",
        "# print(\"-----------------------------------------------TEST LOOP OUTPUT--------------------------------------------------\", file=test_loop_output_file)\n",
        "# print(\"-----------------------------------------------------------------------------------------------------------------\", file=test_loop_output_file)\n",
        "# test_loop_output_file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Qw783gCcBhs_"
      },
      "outputs": [],
      "source": [
        "## HYPER PARAMETERS\n",
        "_LEARNING_RATE = 1.0\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f63m3mjHBhs_"
      },
      "source": [
        "### Quantum Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "_KKJ1joUBhs_"
      },
      "outputs": [],
      "source": [
        "## Create a Linear Entanglement Quantum Model for MNIST Data Classification with three linear entanglement layers of RXX, RYY, and RZZ.\n",
        "quantum_model = linear_entanglement_r3_quantum_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "RdWVyjlNBhtA"
      },
      "outputs": [],
      "source": [
        "quantum_program = classiq.synthesize(quantum_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "BOq6qPE0BhtA"
      },
      "outputs": [],
      "source": [
        "# View Quantum Program on Classiq Platform\n",
        "# classiq.show(quantum_program)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gdd8mDXTBhtA"
      },
      "source": [
        "### Quantum Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "SsDDWwIJBhtA"
      },
      "outputs": [],
      "source": [
        "qnn = QNN(\n",
        "    quantum_program=quantum_program,\n",
        "    execute=execute_fn,\n",
        "    post_process=post_process_fn,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "JmvG6_b2BhtA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "006c494b-955f-499b-ba9b-b80728471ff0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "========================================================================================================================\n",
              "Layer (type (var_name))                  Input Shape          Output Shape         Param #              Trainable\n",
              "========================================================================================================================\n",
              "QNN (QNN)                                [32, 16]             [32, 2]              --                   True\n",
              "├─QLayer (qlayer)                        [32, 16]             [32, 2]              9                    True\n",
              "========================================================================================================================\n",
              "Total params: 9\n",
              "Trainable params: 9\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 0.00\n",
              "========================================================================================================================\n",
              "Input size (MB): 0.00\n",
              "Forward/backward pass size (MB): 0.00\n",
              "Params size (MB): 0.00\n",
              "Estimated Total Size (MB): 0.00\n",
              "========================================================================================================================"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "summary(model=qnn, input_size=(32, 16), verbose=0, col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"], col_width=20, row_settings=[\"var_names\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "NNJ28avgBhtB"
      },
      "outputs": [],
      "source": [
        "# choosing our loss function\n",
        "loss_fn = nn.L1Loss()\n",
        "# choosing our optimizer\n",
        "optimizer = optim.SGD(qnn.parameters(), lr=_LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUtDi6tiBhtB"
      },
      "source": [
        "### Preparing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mY2ZNn7phZAU",
        "outputId": "4676ca3d-6875-4d4e-d801-bc61d758a278"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data/mini_data_1280_bin directory exists.\n",
            "Downloading mnist bin data...\n",
            "Unzipping mnist bin data...\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "# Setup path to data folder\n",
        "data_path = Path(\"data/\")\n",
        "image_path = data_path / \"mini_data_1280_bin\"\n",
        "\n",
        "# If the image folder doesn't exist, download it and prepare it...\n",
        "if image_path.is_dir():\n",
        "    print(f\"{image_path} directory exists.\")\n",
        "else:\n",
        "    print(f\"Did not find {image_path} directory, creating one...\")\n",
        "    image_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Download pizza, steak, sushi data\n",
        "with open(data_path / \"mini_data_1280_bin.zip\", \"wb\") as f:\n",
        "    request = requests.get(\"https://github.com/devilkiller-ag/QNN-MNIST-Classification/raw/main/mini_data_1280_bin.zip\")\n",
        "    print(\"Downloading mnist bin data...\")\n",
        "    f.write(request.content)\n",
        "\n",
        "# Unzip pizza, steak, sushi data\n",
        "with zipfile.ZipFile(data_path / \"mini_data_1280_bin.zip\", \"r\") as zip_ref:\n",
        "    print(\"Unzipping mnist bin data...\")\n",
        "    zip_ref.extractall(image_path)\n",
        "\n",
        "# Remove zip file\n",
        "os.remove(data_path / \"mini_data_1280_bin.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "YXQuouQbg49R"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "train_dir = Path('data/mini_data_1280_bin/mini_data_1280_bin/train')\n",
        "test_dir = Path('data/mini_data_1280_bin/mini_data_1280_bin/test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "UsXSBiPzBhtB"
      },
      "outputs": [],
      "source": [
        "train_dataloader, test_dataloader, class_names = create_dataloaders_from_folders(\n",
        "    train_dir=train_dir,\n",
        "    test_dir=test_dir,\n",
        "    transform=input_transform,\n",
        "    target_transform=target_transform_bin,\n",
        "    batch_size=BATCH_SIZE,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woRGTarVBhtB",
        "outputId": "906d3b95-94c8-46da-b509-27511d43484c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataloaders: (<torch.utils.data.dataloader.DataLoader object at 0x793d18718520>, <torch.utils.data.dataloader.DataLoader object at 0x793c70ee6710>)\n",
            "Length of train dataloader: 80 batches of 32\n",
            "Length of test dataloader: 80 batches of 32\n",
            "Our Dataset have following classes: ['0', '1']\n"
          ]
        }
      ],
      "source": [
        "# Let's check out what we've created\n",
        "print(f\"Dataloaders: {train_dataloader, test_dataloader}\")\n",
        "print(f\"Length of train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}\")\n",
        "print(f\"Length of test dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}\")\n",
        "print(f\"Our Dataset have following classes: {class_names}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YysARR6GBhtB",
        "outputId": "103f0988-22ea-4b8f-9c91-89e42787dd55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image shape: torch.Size([32, 16]) -> [batch_size, pixel_angle]\n",
            "Label shape: torch.Size([32, 2]) -> [batch_size, label_value]\n"
          ]
        }
      ],
      "source": [
        "data, label = next(iter(train_dataloader))\n",
        "\n",
        "print(f\"Image shape: {data.shape} -> [batch_size, pixel_angle]\")\n",
        "print(f\"Label shape: {label.shape} -> [batch_size, label_value]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9YAA5LVBhtC"
      },
      "source": [
        "#### Run Experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhTZscbuBhtC"
      },
      "source": [
        "##### 01. Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fsQqsV9BhtC",
        "outputId": "8ca595d8-a07a-471b-882b-af13ff66b45e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Created SummaryWriter, saving to: runs/2024-01-08/custom_data_1280/linear_entanglement_r3/10_epochs...\n"
          ]
        }
      ],
      "source": [
        "# Create a writer for tracking our experiment\n",
        "writer = create_writer(experiment_name=\"custom_data_1280\", model_name=\"linear_entanglement_r3\", extra=f\"{EPOCHS}_epochs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "45ebcc42056f4690a4e13af968bf3403",
            "3ee2353b51d34bbc89b1a2b9294b3ca6",
            "140270786ac5499ba5dc32ba6f4eed19",
            "ccf70bded5c64f0d8cbb887119f5b9ef",
            "a0fab330b648406e92d319d6830aea9f",
            "d1ce5484314d459eb4b2fd7c0edd0afc",
            "b4692802884d4c50bccef0f8237b5e08",
            "3847784b37bc4b1eafd88f9d1e6fd479",
            "736446e3e2264214a8caab40066a02b4",
            "f36853d5e4764ce88e913b25c24f4ccc",
            "db3d4acb0c1c404b89f959eec1f70b3b"
          ]
        },
        "id": "HH7N17hXBhtC",
        "outputId": "6a23e095-5e8f-4fb1-d4f7-fbb272d69c13"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "45ebcc42056f4690a4e13af968bf3403"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\n",
            "----------\n"
          ]
        }
      ],
      "source": [
        "train_results = train(\n",
        "    model = qnn,\n",
        "    data_loader = train_dataloader,\n",
        "    loss_fn = loss_fn,\n",
        "    optimizer = optimizer,\n",
        "    writer = writer,\n",
        "    epochs = EPOCHS,\n",
        "    device = device\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vIhe8MF5BhtC"
      },
      "outputs": [],
      "source": [
        "# Check out the model results\n",
        "print(train_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDrQsN2vBhtD"
      },
      "outputs": [],
      "source": [
        "write_train_results(experiment_name=\"data_1280_bin\", model_name=\"linear_entanglement_r3\", epochs=EPOCHS, results=train_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULiMikUGBhtD"
      },
      "outputs": [],
      "source": [
        "# %load_ext tensorboard\n",
        "# %tensorboard --logdir runs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTMCHKsfBhtD"
      },
      "source": [
        "##### 02. Save the trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1F0b2yyTBhtD"
      },
      "outputs": [],
      "source": [
        "save_model(\n",
        "    model=qnn,\n",
        "    target_dir='outputs/saved_models',\n",
        "    model_name=f'exp_1_leqmr3_data_1280_bin_epoch{EPOCHS}.pt'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zw0or0MBhtD"
      },
      "source": [
        "##### 03. Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSZIMQohBhtD"
      },
      "outputs": [],
      "source": [
        "test_results = test(\n",
        "    model = qnn,\n",
        "    data_loader = test_dataloader,\n",
        "    device = device\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrhdrI0YBhtD"
      },
      "outputs": [],
      "source": [
        "print(test_results)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "qnn",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "45ebcc42056f4690a4e13af968bf3403": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3ee2353b51d34bbc89b1a2b9294b3ca6",
              "IPY_MODEL_140270786ac5499ba5dc32ba6f4eed19",
              "IPY_MODEL_ccf70bded5c64f0d8cbb887119f5b9ef"
            ],
            "layout": "IPY_MODEL_a0fab330b648406e92d319d6830aea9f"
          }
        },
        "3ee2353b51d34bbc89b1a2b9294b3ca6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1ce5484314d459eb4b2fd7c0edd0afc",
            "placeholder": "​",
            "style": "IPY_MODEL_b4692802884d4c50bccef0f8237b5e08",
            "value": "  0%"
          }
        },
        "140270786ac5499ba5dc32ba6f4eed19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3847784b37bc4b1eafd88f9d1e6fd479",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_736446e3e2264214a8caab40066a02b4",
            "value": 0
          }
        },
        "ccf70bded5c64f0d8cbb887119f5b9ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f36853d5e4764ce88e913b25c24f4ccc",
            "placeholder": "​",
            "style": "IPY_MODEL_db3d4acb0c1c404b89f959eec1f70b3b",
            "value": " 0/10 [00:00&lt;?, ?it/s]"
          }
        },
        "a0fab330b648406e92d319d6830aea9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1ce5484314d459eb4b2fd7c0edd0afc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4692802884d4c50bccef0f8237b5e08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3847784b37bc4b1eafd88f9d1e6fd479": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "736446e3e2264214a8caab40066a02b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f36853d5e4764ce88e913b25c24f4ccc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db3d4acb0c1c404b89f959eec1f70b3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}